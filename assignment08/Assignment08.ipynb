{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习课上内容， 阅读相应论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答以下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  What is autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the differences between greedy search and beam search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans：  \n",
    "在机器翻译的模型中，greedy seach会计算得到最可能的一个单词，然后继续计算得到下一个最可能的单词，然后接着选取出最可能的词，直到完成整个句子的输出。  \n",
    "beam search 计算时会先挑出前n个可能的词，然后再计算每个词对应的下一步中前n个词，依次往下计算选取，直到遇到结束符或者句子输出完成，最后选取概率最大的句子作为结果输出。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the intuition of attention mechanism?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "Attention是用于提升基于RNN（LSTM/GRU）的Encoder+Decoder模型效果的机制（Mechanism），所以一般称为Attention Mechanism。\n",
    "在传统的Encoder-Decoder结构中，Encoder把输入序列编码成统一的语义特征C再Decode。因此，C中必须包含原始序列的所有信息，它的长度就会限制模型性能。\n",
    "Attention机制中，在每个阶段输入不同的c来解决该问题。每个c经过训练后会自动选出与当前要输出的y最相关的上下文信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the disadvantage of word embeding introduced in previous lectures ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans：\n",
    "不能解决一次多义的问题。在不同的语境中，一个词的意思可能是不同的，但在词向量的表示上却是相同的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What is the architecture of ELMo model. (A brief description is enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "简单来说，ELMO可以说是一个双层双向的RNN或LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compared to RNN,  what is the advantage of Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在RNN中计算一个句子，需要一次计算每个词，上一个词计算完成后才能计算下一个词，计算时只能采用串行的方式，而transformer中是直接输入一个句子，其会对每个词的位置进行编码，因此计算时与词之间的前后位置没有关系，计算的时候可以采用并行计算的方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Why we use layer normalizaiton instead of batch normalization in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "batch normalization是从一个batch里选取一个样本中某个词的某个维度进行标准化。 layer normalization 是从一batch中一个样本某个位置的词向量进行标准化。相比批归一化而言，层归一化是对一个中间层进行归一化，不受batch size的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Why we need position embedding in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "transformer中的attention机制没有包含位置信息，各个词在一句话中的不同位置是没有区别的，这与实际是不相符的，以此引入了位置向量，\n",
    "\n",
    "<center>$PE_{(pos,2i)}$=sin(pos/$10000^{2i/d_{model}}$) </br></center>\n",
    "<center>$PE_{(pos,2i+1)}$=cos(pos/$10000^{2i/d_{model}}$) </br></center>\n",
    "其中，PE为二维矩阵，大小跟输入embedding的维度一样，行表示词语，列表示词向量；pos 表示词语在句子中的位置；$d_{model}$表示词向量的维度；i表示词向量的位置。\n",
    "使用sin编码和cos编码的原因是可以得到词语之间的相对位置,因为：\n",
    "<center>sin(α+β)=sinαcosβ+cosαsinβ </br>\n",
    "cos(α+β)=cosαcosβ−sinαsinβ </center>\n",
    "即由sin(pos+k)可以得到，通过线性变换获取后续词语相对当前词语的位置关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Briefly describe what is self-attention and what is multi-head attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "self-attention: 自注意力机制允许输入之间相互交互(\"self\") 然后计算出最需要注意的输入(\"attention\"),所有输入会相互计算并得到一个注意力的评分(softmax)，最后聚合得到输出。 \n",
    "其计算可划分为以下几个步骤:\n",
    "1) 准备输入数据  \n",
    "2) 初始化权重值  \n",
    "3) 推导获得 key, query and value  \n",
    "4) 计算input1的注意力分数  \n",
    "5) 计算softmax  \n",
    "6) 将分数和values相乘  \n",
    "7) 将加权后的所有values 加在一起得到output1  \n",
    "8) 对其他输入重复以上步骤 4–7得到相应的输出   \n",
    "\n",
    "multi-head attention: 是多个Self-Attention结构的结合，每个head学习到在不同表示空间中的特征，最后再组合起来得到输出."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. What is the basic unit of GPT model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Briefly descibe how to use GPT in other NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. What is masked language model in BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "masked language model(MLM)是指在训练的时候随机从输入预料上mask掉一些单词，然后让模型通过上下文预测该单词, 并以此来训练模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What are the inputs of BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "BERT的输入包括三个部分：\n",
    "1) 词向量(token embedding): 表示模型中关于词最主要的信息。  \n",
    "2) 句向量(segment embedding): 用于区分两个句子，每个句子由不同的向量表示。句子末尾都有加[SEP]结尾符，两句拼接开头有[CLS]符。  \n",
    "3) 位置向量(position embedding):Transformer模型不会记住时序，所以引入表示位置的向量。  \n",
    "最后BERT的输入是将上面三部分拼接起来然后送入模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Briely descibe how to use BERT in other NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "1) 句子关系类任务: 比如智能问答、自然语言推理等。和GPT类似，对输入加上一个起始和终结符号，句子之间加个分隔符。对于输出，把第一个起始符号对应的Transformer最后一层位置上面串接一个softmax分类层即可。\n",
    "2) 分类任务： 比如文本分类、情感分析等。与GPT一样，只需要增加起始和终结符号，输出部分和句子关系判断任务类似改造\n",
    "3) 序列标注：比如分词、实体识别、语义标注。输入部分和单句分类一样，只需要输出部分Transformer最后一层每个单词对应位置进行分类。 \n",
    "4) 生成式任务：机器翻译、文本摘要等。结构上需要作相应的改造，可在单个Transformer结构上加装隐层来产生输出。 \n",
    "4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. What are the differences between these three models: GPT, BERT, GPT2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans：  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
