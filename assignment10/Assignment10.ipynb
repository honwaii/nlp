{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.å¤ä¹ ä¸Šè¯¾å†…å®¹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. å›žç­”ä¸€ä¸‹ç†è®ºé¢˜ç›®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What is independent assumption in Naive bayes ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "Naive Bayes is so called because the independence assumptions we have just made are indeed very naive for a model of natural language. The conditional independence assumption states that features are independent of each other given the class.   \n",
    "Definition: X is conditionally independent of Y given Z, if the probability distribution governing X is independent of the value of Y, given the value of Z   \n",
    "$$\\forall(i,j,k) P(X=x_{i}|Y=y_{j},Z=z_{k}) = P(X=x_{i}|Z=z_{k}) $$\n",
    "Which we often write \n",
    "$$P(X|Y,Z) = P(X|Z)$$\n",
    "å®žé™…åº”ç”¨ä¸­ï¼Œç‹¬ç«‹å‡è®¾æ˜¯è¯´å‡è®¾æ•°æ®ä¹‹é—´æ˜¯ç‹¬ç«‹çš„ï¼Œè€Œå®žé™…ä¸Šå¯èƒ½æ˜¯è”åˆçš„ï¼Œä½†é‚£æ ·å¯¹æ•°æ®é›†ç­‰çš„è¦æ±‚æ¯”è¾ƒé«˜ï¼Œè®¡ç®—ä¹Ÿç›¸å¯¹å¤æ‚ï¼Œæ‰€ä»¥å°†å…¶ä½œç‹¬ç«‹å‡è®¾æ¥ç®€å•åŒ–ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What is MAP(maximum a posterior) and ML(maximum likelihood) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "å‡è®¾Dæ˜¯è§‚æµ‹æ•°æ®ï¼ŒHæ˜¯å‡è®¾ç©ºé—´ï¼Œé‚£ä¹ˆæœ‰\n",
    "    $$h_{MAP} = arg \\max_{h \\epsilon H} P(h|D)=arg \\max_{h \\epsilon H} \\frac{P(D|h)P(h)}{P(D)}=arg \\max_{h \\epsilon H}P(D|h)P(h)$$\n",
    "$h_{MAP}$å³ä¸ºæœ€å¤§åŽéªŒæ¦‚çŽ‡ï¼Œè¡¨å¾å‡è®¾ç©ºé—´å†…æœ€å¯èƒ½çš„hçš„æ¦‚çŽ‡ã€‚  \n",
    "ä»¤ä¸Šå¼ä¸­çš„P(h)ä¸ºä¸€ä¸ªå¸¸æ•°ï¼Œå³æ˜¯è¯´å‡è®¾ç©ºé—´å†…çš„æ‰€æœ‰hçš„æ¯”é‡æ˜¯ä¸€æ ·çš„ï¼Œé‚£ä¹ˆåˆ™å¯å°†P(h)ä»Žå¼ä¸­åŽ»æŽ‰ï¼Œè¯¥å¼åˆ™å¯ç®€åŒ–ä¸º:$$h_{ML} = arg \\max_{h \\epsilon H} P(D|h)$$  \n",
    "\n",
    "ç§°$h_{ML}$ä¸ºæœ€å¤§ä¼¼ç„¶(ML)ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is support vector in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "åœ¨æ”¯æŒå‘é‡æœºä¸­ï¼Œè·ç¦»è¶…å¹³é¢æœ€è¿‘çš„ä¸”æ»¡è¶³ä¸€å®šæ¡ä»¶çš„å‡ ä¸ªè®­ç»ƒæ ·æœ¬ç‚¹è¢«ç§°ä¸ºæ”¯æŒå‘é‡.  \n",
    "å‡è®¾è¶…å¹³é¢(w,b)èƒ½å°†è®­ç»ƒæ ·æœ¬æ­£ç¡®åˆ†ç±»,å³å¯¹äºŽ$(x_{i},y_{i})\\epsilon D$, è‹¥$y_{i}=+1$,åˆ™æœ‰$w^{T}x_{i}+b>0$;è‹¥$y_{i}=-1$,åˆ™æœ‰$w^{T}x_{i}+b<0$.ä»¤$$\\left\\{\n",
    "\\begin{aligned}\n",
    "w^{T}x_{i}+b \\geq +1, y_{i}=+1; \\\\\n",
    "w^{T}x_{i}+b \\leq -1, y_{i}=-1. \n",
    "\\end{aligned}\n",
    "\\right.$$\n",
    "è·ç¦»è¶…å¹³é¢æœ€è¿‘çš„å‡ ä¸ªä½¿ä¸Šå¼æˆç«‹çš„è®­ç»ƒæ ·æœ¬ç‚¹å°±æˆä¸º\"æ”¯æŒå‘é‡\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the intuition behind SVM ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "ç»™å®šæœ‰ä¸€æ‰¹è®­ç»ƒé›†,éœ€è¦å°†ä¸åŒç±»åˆ«çš„æ ·æœ¬åˆ†å¼€,è€Œèƒ½å°†è®­ç»ƒæ ·æœ¬åˆ†å¼€çš„è¶…å¹³é¢å¯èƒ½æœ‰å¾ˆå¤š,éœ€è¦é€‰å‡ºæœ€å¥½çš„ä¸€ä¸ª,ç›´è§‚ä¸Šçœ‹åº”è¯¥æ˜¯æ‰¾ä½äºŽæ­£ä¸­é—´çš„åˆ’åˆ†è¶…å¹³é¢,å¦‚å›¾ä¸­çº¢è‰²çš„å¹³é¢.\n",
    "SVMè¦åšçš„å‰§åœºé€šè¿‡å·²æœ‰çš„æ•°æ®æ‰¾åˆ°è¿™ä¸ªè¶…å¹³é¢å°†æ•°æ®åˆ’åˆ†ä¸ºæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬,å½“æœ‰æ–°çš„æ•°æ®è¾“å…¥æ—¶,å¯ä¹Ÿæµ‹åˆ¤æ–­å‡ºè¾“å…¥å±žäºŽæ­£æ ·æœ¬è¿˜æ˜¯è´Ÿæ ·æœ¬. \n",
    "![avatar](./svm-demo.jpg)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Shortly describ what 'random' means in random forest ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "éšæœºæ£®æž—æŒ‡çš„æ˜¯åˆ©ç”¨å¤šæ£µæ ‘å¯¹æ ·æœ¬è¿›è¡Œè®­ç»ƒå¹¶é¢„æµ‹çš„ä¸€ç§åˆ†ç±»å™¨, æ˜¯Baggingçš„ä¸€ä¸ªæ‰©å±•å˜ä½“ã€‚å…¶éšæœºçš„å«ä¹‰ä¸»è¦ä½“çŽ°åœ¨ä¸¤ä¸ªæ–¹é¢ï¼š    \n",
    "1) æ ·æœ¬çš„éšæœº: è®­ç»ƒæ—¶çš„æ ·æœ¬æ˜¯ä»Žåˆå§‹æ ·æœ¬éšæœºé‡‡æ ·å¾—åˆ°çš„ã€‚  \n",
    "2) å±žæ€§çš„éšæœºï¼šè®­ç»ƒæ—¶éšæœºé€‰å–æ ·æœ¬çš„è‹¥å¹²ä¸ªç‰¹æ€§è¿›è¡Œè®­ç»ƒã€‚  \n",
    "å› æ­¤ï¼Œéšæœºæ£®æž—ä¸­åŸºå­¦ä¹ å™¨çš„å¤šæ ·æ€§ä¸ä»…æ¥è‡ªæ ·æœ¬çš„æ‰°åŠ¨ï¼Œè¿˜æ¥è‡ªå±žæ€§çš„æ‰°åŠ¨ï¼Œæœ€ç»ˆä½¿å¾—é›†æˆçš„æ³›åŒ–æ€§èƒ½å¯é€šè¿‡ä¸ªä½“å­¦ä¹ å™¨ä¹‹é—´çš„å·®å¼‚åº¦å¢žåŠ æ¥æå‡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. What cariterion does XGBoost use to find the best split point in a tree ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "å¯»æ‰¾æœ€ä½³åˆ†å‰²ç‚¹çš„å¤§è‡´æ­¥éª¤å¦‚ä¸‹:  \n",
    "1) éåŽ†æ¯ä¸ªç»“ç‚¹çš„æ¯ä¸ªç‰¹å¾ï¼›  \n",
    "2) å¯¹æ¯ä¸ªç‰¹å¾ï¼ŒæŒ‰ç‰¹å¾å€¼å¤§å°å°†ç‰¹å¾å€¼æŽ’åºï¼›  \n",
    "3) çº¿æ€§æ‰«æï¼Œæ‰¾å‡ºæ¯ä¸ªç‰¹å¾çš„æœ€ä½³åˆ†è£‚ç‰¹å¾å€¼ï¼›  \n",
    "4) åœ¨æ‰€æœ‰ç‰¹å¾ä¸­æ‰¾å‡ºæœ€å¥½çš„åˆ†è£‚ç‚¹(åˆ†è£‚åŽå¢žç›Šæœ€å¤§çš„ç‰¹å¾åŠç‰¹å¾å€¼);   \n",
    "å¢žç›Šçš„å®šä¹‰å¦‚ä¸‹ï¼š  \n",
    "$$ðºð‘Žð‘–ð‘›= \\frac {1}{2} [\\frac {ðº_ð¿^2}{ð»_ð¿+ðœ†}+ \\frac{ðº_ð‘…^2}{ð»_ð‘…+ðœ†}âˆ’\\frac {(ðº_ð¿+ðº_ð‘… )^2}{(ð»_ð¿+ð»_ð‘…+ðœ†)}]âˆ’ðœ† $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Practial part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem description: In this part you are going to build a classifier to detect if a piece of news is published by the Xinhua news agency (æ–°åŽç¤¾ï¼‰."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hints:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. Firstly, you have to come up with a way to represent the news. (Vectorize the sentence, you can find different ways to do so online)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from functools import reduce\n",
    "\n",
    "import gensim\n",
    "import jieba\n",
    "import numpy as  np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "def handle_news(stop_words_list: list):\n",
    "    essays_path = './news_data.csv'\n",
    "    contents = pd.read_csv(essays_path, encoding='gb18030', usecols=[\"source\", \"content\"])\n",
    "    news = []\n",
    "    labels = []\n",
    "    count = 0\n",
    "    for each in contents.iterrows():\n",
    "        content = str(each[1]['content']).strip()\n",
    "        source = str(each[1]['source']).strip()\n",
    "        if content == 'nan':\n",
    "            continue\n",
    "        if content is None or not isinstance(content, str):\n",
    "            continue\n",
    "        content = handle_doc(content, stop_words_list)\n",
    "        news.append(content)\n",
    "        if 'æ–°åŽç¤¾' in source:\n",
    "            labels.append('1')\n",
    "        else:\n",
    "            labels.append('0')\n",
    "        count += 1\n",
    "        if count % 2000 == 0:\n",
    "            print('handle docs: ' + str(count))\n",
    "\n",
    "    with open(\"./news.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.writelines(news)\n",
    "        f.flush()\n",
    "        f.close()\n",
    "    with open(\"./labels.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.writelines(labels)\n",
    "        f.flush()\n",
    "        f.close()\n",
    "\n",
    "    # print(\"èŽ·å–åˆ°çš„æ–‡ç« æ•°:\" + str(len(essays)))\n",
    "    # print(\"æ–°åŽç¤¾çš„æ–‡ç« æ•°:\" + str(count))\n",
    "    return\n",
    "\n",
    "\n",
    "def split_content(content: str, stop_words: list):\n",
    "    simpled = ''\n",
    "    s = content.replace(\"æ–°åŽç¤¾\", \"\")\n",
    "    s = content.replace(\"\\n\", \"\")\n",
    "    if s == \"\":\n",
    "        return simpled\n",
    "    segs = jieba.cut(s)\n",
    "    for seg in segs:\n",
    "        if seg in stop_words:\n",
    "            continue\n",
    "        simpled += seg + \" \"\n",
    "    return simpled\n",
    "\n",
    "\n",
    "def handle_doc(doc: str, stop_words_list: list):\n",
    "    doc = doc.replace(\"\\n\", \"ã€‚\").strip()\n",
    "    doc = doc.replace(r\"\\n\", \"ã€‚\").strip()\n",
    "    doc = doc.replace(\"\\r\", \"ã€‚\").strip()\n",
    "    doc = doc.replace(\"\\t\", \"ã€‚\").strip()\n",
    "    doc = doc.replace(\"æ–°åŽç¤¾\", \"\").strip()\n",
    "    content = split_content(doc, stop_words_list) + \"\\n\"\n",
    "    return content\n",
    "\n",
    "\n",
    "def get_word_vector(word: str, word_vector_model: Word2Vec):\n",
    "    try:\n",
    "        word_vector = word_vector_model[word]\n",
    "    except KeyError:\n",
    "        word_vector = np.zeros(word_vector_model.vector_size)\n",
    "    return word_vector\n",
    "\n",
    "\n",
    "def load_word_vector_model(path: str, self_trained: bool):\n",
    "    print(\"åŠ è½½çš„è¯å‘é‡çš„è·¯å¾„: \" + path)\n",
    "    # åŠ è½½gloveè½¬æ¢çš„æ¨¡åž‹: ä¿å­˜çš„ä¸ºæ–‡æœ¬å½¢å¼\n",
    "    if self_trained:\n",
    "        word_embedding = gensim.models.Word2Vec.load(path)\n",
    "    else:\n",
    "        word_embedding = KeyedVectors.load_word2vec_format(path)\n",
    "    print('load finished.')\n",
    "    return word_embedding\n",
    "\n",
    "\n",
    "def generate_doc_vector(doc: str, word_vec_model: Word2Vec):\n",
    "    words = doc.split(\" \")\n",
    "    word_vec = np.zeros(word_vec_model.vector_size)\n",
    "    for word in words:\n",
    "        word_vec += get_word_vector(word, word_vec_model)\n",
    "    word_vec = word_vec / len(words)\n",
    "    return word_vec\n",
    "\n",
    "\n",
    "def compute_docs_vec(docs: list, model):\n",
    "    return np.row_stack([generate_doc_vector(doc, model) for doc in docs])\n",
    "\n",
    "\n",
    "def load_docs_labels(model):\n",
    "    with open('./news.txt', 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        docs = [str(line) for line in lines]\n",
    "    with open('./labels.txt', 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        labels_str = str(lines[0]).strip()\n",
    "        labels = [int(label) for label in labels_str]\n",
    "\n",
    "    docs_vec = compute_docs_vec(docs, model)\n",
    "    labels = np.asarray(labels)\n",
    "    return docs_vec, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. Secondly,  pick a machine learning algorithm that you think is suitable for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŠ è½½çš„è¯å‘é‡çš„è·¯å¾„: ./word_embedding_model_100\n",
      "load finished.\n",
      "é¢„æµ‹çš„æ–‡ç« :ç½‘æ˜“ä½“è‚²4æœˆ16æ—¥æŠ¥é“ï¼š\n",
      "\n",
      "4æœˆ14æ—¥ï¼ŒäºŽæ±‰è¶…åœ¨å¹¿å·žæ¶‚æ”¹è½¦ç‰Œï¼Œè¯¥äº‹ä»¶è¢«ç½‘å‹å½•åˆ¶åŽä¸Šä¼ ç½‘ç»œï¼Œéšå³å¼•å‘è½©ç„¶å¤§æ³¢ã€‚\n",
      "\n",
      "å½“æ™šï¼Œæ’å¤§å‘å…¬å‘Šå®£å¸ƒå¼€é™¤äºŽæ±‰è¶…ï¼Œä¹‹åŽå¾·å›½è½¬ä¼šå¸‚åœºç½‘ä¹Ÿå°†äºŽæ±‰è¶…å˜ä¸ºè‡ªç”±èº«ã€‚\n",
      "\n",
      "å¯¹äºŽæ’å¤§çš„åšæ³•ï¼Œéƒæµ·ä¸œå†æ¬¡å‘è¯ï¼šâ€œéƒæµ·ä¸œåœ¨è¿™é‡Œè·Ÿè®¸å®¶å°è¯´ä¸€å£°ï¼Œå°Šé‡ä¸€ä¸‹åŠ³åŠ¨æ³•ï¼Œè¿™ä¸ªè¡Œä¸ºå¤Ÿä¸å¤Ÿè§£é™¤åˆåŒçš„ç¨‹åº¦ï¼Œåˆ«æŠŠè¶³çƒè¿åŠ¨å‘˜éƒ½å½“æˆå·¥å…·ï¼Œç»™è‡ªå·±ç•™ç‚¹åŽè·¯ã€‚â€\n",
      "é¢„æµ‹ç»“æžœ:\n",
      "è¯¥æ–°é—»éžæ–°åŽç¤¾å‘å¸ƒ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:80: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_model(model=None, name=None):\n",
    "    x, y = load_docs_labels(word_vec_model)\n",
    "    train_idx, test_idx = train_test_split(range(len(y)), test_size=0.2, stratify=y)\n",
    "    train_x = x[train_idx, :]\n",
    "    train_y = y[train_idx]\n",
    "    test_x = x[test_idx, :]\n",
    "    test_y = y[test_idx]\n",
    "    if model is None:\n",
    "        model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "        name = 'LogisticRegression'\n",
    "    model.fit(train_x, train_y)\n",
    "    print(\"model: \" + name)\n",
    "    print(\"Training set score: {:.3f}\".format(model.score(train_x, train_y)))\n",
    "    print(\"Test set score: {:.3f}\".format(model.score(test_x, test_y)))\n",
    "    y_pred = model.predict(test_x)\n",
    "    eval = eval_model(test_y, y_pred, np.asarray([0, 1]))\n",
    "    print(eval)\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(doc, model):\n",
    "    doc_vec = generate_doc_vector(doc, word_vec_model)\n",
    "    doc_vec = np.asarray(doc_vec).reshape(1, -1)\n",
    "    y = model['lr'].predict(doc_vec)\n",
    "    if y[0] == 0:\n",
    "        return 'è¯¥æ–°é—»éžæ–°åŽç¤¾å‘å¸ƒ'\n",
    "    else:\n",
    "        return 'è¯¥æ–°é—»ç”±æ–°åŽç¤¾å‘å¸ƒ'\n",
    "\n",
    "\n",
    "# è®¡ç®—å„é¡¹è¯„ä»·æŒ‡æ ‡\n",
    "def eval_model(y_true, y_pred, labels):\n",
    "    # è®¡ç®—æ¯ä¸ªåˆ†ç±»çš„Precision, Recall, f1, support\n",
    "    p, r, f1, s = precision_recall_fscore_support(y_true, y_pred)\n",
    "    # è®¡ç®—æ€»ä½“çš„å¹³å‡Precision, Recall, f1, support\n",
    "    tot_p = np.average(p, weights=s)\n",
    "    tot_r = np.average(r, weights=s)\n",
    "    tot_f1 = np.average(f1, weights=s)\n",
    "    tot_s = np.sum(s)\n",
    "    res1 = pd.DataFrame({\n",
    "        u'Label': labels,\n",
    "        u'Precision': p,\n",
    "        u'Recall': r,\n",
    "        u'F1': f1,\n",
    "        u'Support': s\n",
    "    })\n",
    "    res2 = pd.DataFrame({\n",
    "        u'Label': [u'æ€»ä½“'],\n",
    "        u'Precision': [tot_p],\n",
    "        u'Recall': [tot_r],\n",
    "        u'F1': [tot_f1],\n",
    "        u'Support': [tot_s]\n",
    "    })\n",
    "    res2.index = [999]\n",
    "    res = pd.concat([res1, res2])\n",
    "    return res[[u'Label', u'Precision', u'Recall', u'F1', u'Support']]\n",
    "\n",
    "\n",
    "def save_model(model, output_dir):\n",
    "    model_file = os.path.join(output_dir, u'model.pkl')\n",
    "    with open(model_file, 'wb') as outfile:\n",
    "        pickle.dump({\n",
    "            'y_encoder': np.asarray([0, 1]),\n",
    "            'lr': model\n",
    "        }, outfile)\n",
    "    return\n",
    "\n",
    "\n",
    "def load_model(path):\n",
    "    with open(path + 'model.pkl', 'rb') as infile:\n",
    "        lr_model = pickle.load(infile)\n",
    "    return lr_model\n",
    "\n",
    "\n",
    "stop_words = open(u'stopwords.txt', \"r\", encoding=\"utf-8\").readlines()\n",
    "stop_words_list = [line.strip() for line in stop_words]\n",
    "# # handle_news(stop_words_list)\n",
    "word_vec_model = load_word_vector_model(path='./word_embedding_model_100', self_trained=True)\n",
    "model = train_model()\n",
    "save_model(model, './')\n",
    "with open('./news_demo.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    doc = reduce(lambda x, y: x + y, lines)\n",
    "    print(\"é¢„æµ‹çš„æ–‡ç« :\" + doc)\n",
    "    doc = handle_doc(doc, stop_words_list)\n",
    "    f.close()\n",
    "model = load_model('./')\n",
    "result = predict(doc, model)\n",
    "print('é¢„æµ‹ç»“æžœ:\\n'+result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! You have completed all assignments in this week. The question below is optional. If you still have time, why don't try it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try differnt machine learning algorithms with different combinations of parameters in the practical part, and compare their performances (Better use some visualization techiniques)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:80: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: LogisticRegression\n",
      "Training set score: 0.928\n",
      "Test set score: 0.928\n",
      "    Label  Precision    Recall        F1  Support\n",
      "0       0   0.712690  0.418854  0.527621     1676\n",
      "1       1   0.940704  0.982015  0.960915    15735\n",
      "999    æ€»ä½“   0.918755  0.927804  0.919206    17411\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:80: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: GaussianNB\n",
      "Training set score: 0.728\n",
      "Test set score: 0.725\n",
      "    Label  Precision    Recall        F1  Support\n",
      "0       0   0.239592  0.855012  0.374298     1676\n",
      "1       1   0.978740  0.710963  0.823633    15735\n",
      "999    æ€»ä½“   0.907589  0.724829  0.780380    17411\n"
     ]
    }
   ],
   "source": [
    "def compare_model():\n",
    "    model_1 = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "    train_model(model_1, name='LogisticRegression')\n",
    "    print('--------------------------------------')\n",
    "    model_2 = GaussianNB()\n",
    "    train_model(model_2, name='GaussianNB')\n",
    "    return\n",
    "compare_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
