{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习上课内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答一下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What is independent assumption in Naive bayes ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "Naive Bayes is so called because the independence assumptions we have just made are indeed very naive for a model of natural language. The conditional independence assumption states that features are independent of each other given the class.   \n",
    "Definition: X is conditionally independent of Y given Z, if the probability distribution governing X is independent of the value of Y, given the value of Z   \n",
    "$$\\forall(i,j,k) P(X=x_{i}|Y=y_{j},Z=z_{k}) = P(X=x_{i}|Z=z_{k}) $$\n",
    "Which we often write \n",
    "$$P(X|Y,Z) = P(X|Z)$$\n",
    "实际应用中，独立假设是说假设数据之间是独立的，而实际上可能是联合的，但那样对数据集等的要求比较高，计算也相对复杂，所以将其作独立假设来简单化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What is MAP(maximum a posterior) and ML(maximum likelihood) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "假设D是观测数据，H是假设空间，那么有\n",
    "    $$h_{MAP} = arg \\max_{h \\epsilon H} P(h|D)=arg \\max_{h \\epsilon H} \\frac{P(D|h)P(h)}{P(D)}=arg \\max_{h \\epsilon H}P(D|h)P(h)$$\n",
    "$h_{MAP}$即为最大后验概率，表征假设空间内最可能的h的概率。  \n",
    "令上式中的P(h)为一个常数，即是说假设空间内的所有h的比重是一样的，那么则可将P(h)从式中去掉，该式则可简化为:$$h_{ML} = arg \\max_{h \\epsilon H} P(D|h)$$  \n",
    "\n",
    "称$h_{ML}$为最大似然(ML)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is support vector in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "在支持向量机中，距离超平面最近的且满足一定条件的几个训练样本点被称为支持向量.  \n",
    "假设超平面(w,b)能将训练样本正确分类,即对于$(x_{i},y_{i})\\epsilon D$, 若$y_{i}=+1$,则有$w^{T}x_{i}+b>0$;若$y_{i}=-1$,则有$w^{T}x_{i}+b<0$.令$$\\left\\{\n",
    "\\begin{aligned}\n",
    "w^{T}x_{i}+b \\geq +1, y_{i}=+1; \\\\\n",
    "w^{T}x_{i}+b \\leq -1, y_{i}=-1. \n",
    "\\end{aligned}\n",
    "\\right.$$\n",
    "距离超平面最近的几个使上式成立的训练样本点就成为\"支持向量\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the intuition behind SVM ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "给定有一批训练集,需要将不同类别的样本分开,而能将训练样本分开的超平面可能有很多,需要选出最好的一个,直观上看应该是找位于正中间的划分超平面,如图中红色的平面.\n",
    "SVM要做的剧场通过已有的数据找到这个超平面将数据划分为正样本和负样本,当有新的数据输入时,可也测判断出输入属于正样本还是负样本. \n",
    "![avatar](./svm-demo.jpg)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Shortly describ what 'random' means in random forest ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "随机森林指的是利用多棵树对样本进行训练并预测的一种分类器, 是Bagging的一个扩展变体。其随机的含义主要体现在两个方面：    \n",
    "1) 样本的随机: 训练时的样本是从初始样本随机采样得到的。  \n",
    "2) 属性的随机：训练时随机选取样本的若干个特性进行训练。  \n",
    "因此，随机森林中基学习器的多样性不仅来自样本的扰动，还来自属性的扰动，最终使得集成的泛化性能可通过个体学习器之间的差异度增加来提升。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. What cariterion does XGBoost use to find the best split point in a tree ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "寻找最佳分割点的大致步骤如下:  \n",
    "1) 遍历每个结点的每个特征；  \n",
    "2) 对每个特征，按特征值大小将特征值排序；  \n",
    "3) 线性扫描，找出每个特征的最佳分裂特征值；  \n",
    "4) 在所有特征中找出最好的分裂点(分裂后增益最大的特征及特征值);   \n",
    "增益的定义如下：  \n",
    "$$𝐺𝑎𝑖𝑛= \\frac {1}{2} [\\frac {𝐺_𝐿^2}{𝐻_𝐿+𝜆}+ \\frac{𝐺_𝑅^2}{𝐻_𝑅+𝜆}−\\frac {(𝐺_𝐿+𝐺_𝑅 )^2}{(𝐻_𝐿+𝐻_𝑅+𝜆)}]−𝜆 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Practial part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem description: In this part you are going to build a classifier to detect if a piece of news is published by the Xinhua news agency (新华社）."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hints:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. Firstly, you have to come up with a way to represent the news. (Vectorize the sentence, you can find different ways to do so online)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from functools import reduce\n",
    "\n",
    "import gensim\n",
    "import jieba\n",
    "import numpy as  np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "def handle_news(stop_words_list: list):\n",
    "    essays_path = './news_data.csv'\n",
    "    contents = pd.read_csv(essays_path, encoding='gb18030', usecols=[\"source\", \"content\"])\n",
    "    news = []\n",
    "    labels = []\n",
    "    count = 0\n",
    "    for each in contents.iterrows():\n",
    "        content = str(each[1]['content']).strip()\n",
    "        source = str(each[1]['source']).strip()\n",
    "        if content == 'nan':\n",
    "            continue\n",
    "        if content is None or not isinstance(content, str):\n",
    "            continue\n",
    "        content = handle_doc(content, stop_words_list)\n",
    "        news.append(content)\n",
    "        if '新华社' in source:\n",
    "            labels.append('1')\n",
    "        else:\n",
    "            labels.append('0')\n",
    "        count += 1\n",
    "        if count % 2000 == 0:\n",
    "            print('handle docs: ' + str(count))\n",
    "\n",
    "    with open(\"./news.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.writelines(news)\n",
    "        f.flush()\n",
    "        f.close()\n",
    "    with open(\"./labels.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.writelines(labels)\n",
    "        f.flush()\n",
    "        f.close()\n",
    "\n",
    "    # print(\"获取到的文章数:\" + str(len(essays)))\n",
    "    # print(\"新华社的文章数:\" + str(count))\n",
    "    return\n",
    "\n",
    "\n",
    "def split_content(content: str, stop_words: list):\n",
    "    simpled = ''\n",
    "    s = content.replace(\"新华社\", \"\")\n",
    "    s = content.replace(\"\\n\", \"\")\n",
    "    if s == \"\":\n",
    "        return simpled\n",
    "    segs = jieba.cut(s)\n",
    "    for seg in segs:\n",
    "        if seg in stop_words:\n",
    "            continue\n",
    "        simpled += seg + \" \"\n",
    "    return simpled\n",
    "\n",
    "\n",
    "def handle_doc(doc: str, stop_words_list: list):\n",
    "    doc = doc.replace(\"\\n\", \"。\").strip()\n",
    "    doc = doc.replace(r\"\\n\", \"。\").strip()\n",
    "    doc = doc.replace(\"\\r\", \"。\").strip()\n",
    "    doc = doc.replace(\"\\t\", \"。\").strip()\n",
    "    doc = doc.replace(\"新华社\", \"\").strip()\n",
    "    content = split_content(doc, stop_words_list) + \"\\n\"\n",
    "    return content\n",
    "\n",
    "\n",
    "def get_word_vector(word: str, word_vector_model: Word2Vec):\n",
    "    try:\n",
    "        word_vector = word_vector_model[word]\n",
    "    except KeyError:\n",
    "        word_vector = np.zeros(word_vector_model.vector_size)\n",
    "    return word_vector\n",
    "\n",
    "\n",
    "def load_word_vector_model(path: str, self_trained: bool):\n",
    "    print(\"加载的词向量的路径: \" + path)\n",
    "    # 加载glove转换的模型: 保存的为文本形式\n",
    "    if self_trained:\n",
    "        word_embedding = gensim.models.Word2Vec.load(path)\n",
    "    else:\n",
    "        word_embedding = KeyedVectors.load_word2vec_format(path)\n",
    "    print('load finished.')\n",
    "    return word_embedding\n",
    "\n",
    "\n",
    "def generate_doc_vector(doc: str, word_vec_model: Word2Vec):\n",
    "    words = doc.split(\" \")\n",
    "    word_vec = np.zeros(word_vec_model.vector_size)\n",
    "    for word in words:\n",
    "        word_vec += get_word_vector(word, word_vec_model)\n",
    "    word_vec = word_vec / len(words)\n",
    "    return word_vec\n",
    "\n",
    "\n",
    "def compute_docs_vec(docs: list, model):\n",
    "    return np.row_stack([generate_doc_vector(doc, model) for doc in docs])\n",
    "\n",
    "\n",
    "def load_docs_labels(model):\n",
    "    with open('./news.txt', 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        docs = [str(line) for line in lines]\n",
    "    with open('./labels.txt', 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        labels_str = str(lines[0]).strip()\n",
    "        labels = [int(label) for label in labels_str]\n",
    "\n",
    "    docs_vec = compute_docs_vec(docs, model)\n",
    "    labels = np.asarray(labels)\n",
    "    return docs_vec, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. Secondly,  pick a machine learning algorithm that you think is suitable for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载的词向量的路径: ./word_embedding_model_100\n",
      "load finished.\n",
      "预测的文章:网易体育4月16日报道：\n",
      "\n",
      "4月14日，于汉超在广州涂改车牌，该事件被网友录制后上传网络，随即引发轩然大波。\n",
      "\n",
      "当晚，恒大发公告宣布开除于汉超，之后德国转会市场网也将于汉超变为自由身。\n",
      "\n",
      "对于恒大的做法，郝海东再次发话：“郝海东在这里跟许家印说一声，尊重一下劳动法，这个行为够不够解除合同的程度，别把足球运动员都当成工具，给自己留点后路。”\n",
      "预测结果:\n",
      "该新闻非新华社发布\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:80: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_model(model=None, name=None):\n",
    "    x, y = load_docs_labels(word_vec_model)\n",
    "    train_idx, test_idx = train_test_split(range(len(y)), test_size=0.2, stratify=y)\n",
    "    train_x = x[train_idx, :]\n",
    "    train_y = y[train_idx]\n",
    "    test_x = x[test_idx, :]\n",
    "    test_y = y[test_idx]\n",
    "    if model is None:\n",
    "        model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "        name = 'LogisticRegression'\n",
    "    model.fit(train_x, train_y)\n",
    "    print(\"model: \" + name)\n",
    "    print(\"Training set score: {:.3f}\".format(model.score(train_x, train_y)))\n",
    "    print(\"Test set score: {:.3f}\".format(model.score(test_x, test_y)))\n",
    "    y_pred = model.predict(test_x)\n",
    "    eval = eval_model(test_y, y_pred, np.asarray([0, 1]))\n",
    "    print(eval)\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(doc, model):\n",
    "    doc_vec = generate_doc_vector(doc, word_vec_model)\n",
    "    doc_vec = np.asarray(doc_vec).reshape(1, -1)\n",
    "    y = model['lr'].predict(doc_vec)\n",
    "    if y[0] == 0:\n",
    "        return '该新闻非新华社发布'\n",
    "    else:\n",
    "        return '该新闻由新华社发布'\n",
    "\n",
    "\n",
    "# 计算各项评价指标\n",
    "def eval_model(y_true, y_pred, labels):\n",
    "    # 计算每个分类的Precision, Recall, f1, support\n",
    "    p, r, f1, s = precision_recall_fscore_support(y_true, y_pred)\n",
    "    # 计算总体的平均Precision, Recall, f1, support\n",
    "    tot_p = np.average(p, weights=s)\n",
    "    tot_r = np.average(r, weights=s)\n",
    "    tot_f1 = np.average(f1, weights=s)\n",
    "    tot_s = np.sum(s)\n",
    "    res1 = pd.DataFrame({\n",
    "        u'Label': labels,\n",
    "        u'Precision': p,\n",
    "        u'Recall': r,\n",
    "        u'F1': f1,\n",
    "        u'Support': s\n",
    "    })\n",
    "    res2 = pd.DataFrame({\n",
    "        u'Label': [u'总体'],\n",
    "        u'Precision': [tot_p],\n",
    "        u'Recall': [tot_r],\n",
    "        u'F1': [tot_f1],\n",
    "        u'Support': [tot_s]\n",
    "    })\n",
    "    res2.index = [999]\n",
    "    res = pd.concat([res1, res2])\n",
    "    return res[[u'Label', u'Precision', u'Recall', u'F1', u'Support']]\n",
    "\n",
    "\n",
    "def save_model(model, output_dir):\n",
    "    model_file = os.path.join(output_dir, u'model.pkl')\n",
    "    with open(model_file, 'wb') as outfile:\n",
    "        pickle.dump({\n",
    "            'y_encoder': np.asarray([0, 1]),\n",
    "            'lr': model\n",
    "        }, outfile)\n",
    "    return\n",
    "\n",
    "\n",
    "def load_model(path):\n",
    "    with open(path + 'model.pkl', 'rb') as infile:\n",
    "        lr_model = pickle.load(infile)\n",
    "    return lr_model\n",
    "\n",
    "\n",
    "stop_words = open(u'stopwords.txt', \"r\", encoding=\"utf-8\").readlines()\n",
    "stop_words_list = [line.strip() for line in stop_words]\n",
    "# # handle_news(stop_words_list)\n",
    "word_vec_model = load_word_vector_model(path='./word_embedding_model_100', self_trained=True)\n",
    "model = train_model()\n",
    "save_model(model, './')\n",
    "with open('./news_demo.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    doc = reduce(lambda x, y: x + y, lines)\n",
    "    print(\"预测的文章:\" + doc)\n",
    "    doc = handle_doc(doc, stop_words_list)\n",
    "    f.close()\n",
    "model = load_model('./')\n",
    "result = predict(doc, model)\n",
    "print('预测结果:\\n'+result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! You have completed all assignments in this week. The question below is optional. If you still have time, why don't try it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try differnt machine learning algorithms with different combinations of parameters in the practical part, and compare their performances (Better use some visualization techiniques)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:80: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: LogisticRegression\n",
      "Training set score: 0.928\n",
      "Test set score: 0.928\n",
      "    Label  Precision    Recall        F1  Support\n",
      "0       0   0.712690  0.418854  0.527621     1676\n",
      "1       1   0.940704  0.982015  0.960915    15735\n",
      "999    总体   0.918755  0.927804  0.919206    17411\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:80: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: GaussianNB\n",
      "Training set score: 0.728\n",
      "Test set score: 0.725\n",
      "    Label  Precision    Recall        F1  Support\n",
      "0       0   0.239592  0.855012  0.374298     1676\n",
      "1       1   0.978740  0.710963  0.823633    15735\n",
      "999    总体   0.907589  0.724829  0.780380    17411\n"
     ]
    }
   ],
   "source": [
    "def compare_model():\n",
    "    model_1 = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "    train_model(model_1, name='LogisticRegression')\n",
    "    print('--------------------------------------')\n",
    "    model_2 = GaussianNB()\n",
    "    train_model(model_2, name='GaussianNB')\n",
    "    return\n",
    "compare_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
